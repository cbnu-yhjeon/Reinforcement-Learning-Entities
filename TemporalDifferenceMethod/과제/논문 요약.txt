내비게이션 애플리케이션을 위한 강화 학습에 대한 시뮬레이션 연구


Abstract :

이 논문에서는 강화, 비파라미터 기반 학습 및 의사 결정 방법인 Q-러닝을 구현하는 데 기여했습니다.
시뮬레이션을 통해 Q-러닝 알고리즘을 공식화하고 시연했습니다. 

이 작업에는 의사 코드의 공식화 및애플리케이션을 고려한 알고리즘 개발이 포함됩니다. 

강화학습의 적용은 개념적 농업 분야를 통해 시뮬레이션되며, 로봇에게 나무에 도달하여 
최종적으로 과일을 저장 지점(목표)으로 배달하라는 명령을 내립니다. 

우리의 연구 결과는 할인율(Discount Factor  : γ, gamma[감마] )와 학습률(learning rate: a, alpha[알파])가 
가장 중요하다는 것 결과를 보여줍니다.

 할인율(Discount Factor  : γ, gamma[감마] )과 학습률(learning rate: a, alpha[알파])는 
특정 애플리케이션을 위한 Q러닝 기반 강화 알고리즘을 개발할 때 고려해야 할 두 가지 중요한 매개변수입니다.

할인율(Discount Factor  : γ, gamma[감마] )와 학습률(learning rate: a, alpha[알파])의 최적값을 시뮬레이션 연구를 통해 설정했습니다.


keywrods  :강화 학습 (Reinforcement Learning), Q-러닝 (Q-learning), 로봇 내비게이션 (Robot Navigation), 
소프트 컴퓨팅(Soft Computing),  자동화 및 제어 (Automation and Control)


eg,{
 본 논문에서의 할인율(Y, gamma)의 최적값 : 0.75, 학습률(Learning rate , alpha) 의 사용 값 :0.1 
 - 강화 학습 (Reinforcement Learning) :  이 논문에서는 로봇(에이전트)이 농업 환경에서 나무까지 도달하고 과일을 수확 지점(목표)으로 옮기는 경로를 학습하는 데 강화 학습을 사용합니다.

 - Q-러닝 (Q-learning) :이 논문은 Q-러닝 알고리즘의 구현과 시뮬레이션에 중점을 두고 있으며, 학습률(α)과 할인율(γ)이라는 파라미터가 학습 결과에 미치는 영향을 연구합니다.
 - 로봇 내비게이션 (Robot Navigation) : 이 논문에서는 시뮬레이션 환경(가상의 농업 필드)에서 로봇이 나무들 사이를 이동하여 최종 목적지(저장소)까지 도달하는 경로를 찾는 문제에 Q-러닝을 적용합니다.
 - 소프트 컴퓨팅(Soft Computing) : 퍼지 이론, 신경망, 유전 알고리즘 등과 같이 불확실하고 부정확하며 부분적인 진실을 다루는 데 유용한 계산 방법론들을 통칭합니다. 강화 학습, 특히 Q-러닝은 불확실한 환경에서 학습하고 최적의 행동을 찾아나가는 과정에서 소프트 컴퓨팅 기법과 관련될 수 있습니다.

퍼지 이론(Fuzzy Theory / Fuzzy Logic): : 참(True)' 또는 '거짓(False)'의 명확한 두 가지 값 대신, '어느 정도 참이다' 또는 '어느 정도 ~에 속한다'와 같이 모호하거나 불확실한 정도를 다루는 이론

신경망(Neural Networks / Artificial Neural Networks): 인간의 뇌를 구성하는 신경 세포(뉴런)의 연결 구조를 모방하여 만든 컴퓨팅 모델입니다. 여러 개의 노드(인공 뉴런)들이 서로 연결되어 신호를 주고받으며 정보를 처리합니다.

유전 알고리즘(Genetic Algorithms): 생물의 진화 과정, 특히 다윈의 자연 선택설에 기반한 최적화 알고리즘입니다. 문제의 해(solution)들을 하나의 개체(유전자)로 표현하고, 이 개체 집단에 대해 선택(selection), 교차(crossover), 변이(mutation)와 같은 유전적 연산을 적용하여 더 좋은 해를 찾아나갑니다.

}



1. BACKGROUND AND INTRODUCTION

의사결정은 작업을 조직하고, 계획하고, 실행하고, 완수하는 데 유용합니다. 의사결정은 다양한 분야에서 수많은 응용 사례를 가지고 있습니다. 마르코프 결정 과정(Markov decision process)은 오직 현재 상태에 따라 행동의 효과가 결정된다는 마르코프 속성에 기반합니다. 마르코프 결정 과정에서의 행동은 결정론적 행동(deterministic actions)과 확률론적 행동(stochastic actions)이라는 두 가지 형태로 나타낼 수 있습니다. 결정론적 행동에서는 모든 행동과 상태에 대해 새로운 상태가 정의됩니다. 얻어진 보상은 합산되지만 최종 결과는 불분명할 수 있습니다. 확률론적 분포에서는 모든 행동과 상태에 대해 다음 상태의 확률 분포가 명시됩니다. 기대값과 최악의 경우에 초점을 맞추면 의사결정 과정에서 근사치와 샘플링을 적절히 활용할 수 있습니다.   

베이즈 정리(Baye's theorem)는 파라미터 기반 의사결정 모델의 개념을 이해하는 데 중요한 역할을 합니다. 이 정리는 모델링 및 파라미터 값과 관련된 모든 불확실성을 고려하는 데 유용합니다. 

최대우도법(Maximum likelihood)은 파라미터 값을 고정하는 과정이 포함되므로 대부분의 의사결정 분석을 전담합니다. 

베이지안 접근법(Bayesian approach)은 개별 파라미터의 범위를 명시하여 사전 정보를 구현할 수 있다는 장점이 있습니다.   

비모수적(Non-parameter based) 의사결정 모델링은 인공 신경망 및 의사결정 트리에서 더 나은 성능을 보입니다. 이 경우 통계적 분포를 가정하는데, 이는 여러 출처의 데이터와 호환되지 않을 수 있습니다. 데이터 분포에 대한 가정은 비모수적 의사결정 모델에서는 이루어지지 않으므로 오류를 피할 수 있습니다.   

강화 학습(Reinforcement learning)은 상황을 행동으로 변환하고 보상 점수를 최대화하기 위해 적용되는 방법론적 접근 방식의 한 종류입니다. 에이전트(agent)는 스스로 행동을 수행하는 방법과 어떤 행동이 가장 많은 보상을 가져올지 학습해야 합니다. 이를 위해서는 선호도, 행동 기록, 그리고 이러한 행동을 수행할 특정 시간을 명시해야 합니다. 강화 학습은 컴퓨터가 플레이하는 보드 게임, 엘리베이터 제어, 네트워크 라우팅, 데이터 마이닝, 로봇 제어, 음성 인식, 생물정보학, 웹 및 텍스트 데이터 처리 등에서 훌륭한 응용 분야를 가지고 있습니다. 에이전트가 보상을 잃고 있을 때 이를 알리고 승리를 쟁취할 다른 대안적인 방법을 제안합니다.   

이 논문은 강화 학습 알고리즘의 유용성을 광범위하게 연구합니다. 특히, Q-러닝(Q-learning) 알고리즘을 연구하고 관련된 파라미터의 최적 값에 대한 제안을 합니다. 그 결과는 시뮬레이션을 통해 제시되었습니다

eg,{
-  마르코프 결정 과정(Markov decision process) 이란 ?
마르코프 결정 과정 (Markov Decision Process, MDP):

개념: 순차적인 의사결정 문제를 수학적으로 모델링하는 프레임워크입니다. 에이전트(결정자)가 특정 **상태(state)**에서 어떤 **행동(action)**을 취하면, 정해진 **확률(transition probability)**에 따라 다음 상태로 전이되고 **보상(reward)**을 받습니다. 중요한 특징은 '마르코프 속성'인데, 이는 다음 상태와 보상이 오직 현재 상태와 현재 행동에 의해서만 결정되고 과거 이력과는 무관하다는 것입니다. [source: 8]
목표: 에이전트가 장기적으로 누적 보상을 최대화하는 최적의 행동 전략(policy)을 찾는 것입니다.
관련성: 강화 학습 문제, 특히 논문에서 다루는 Q-러닝은 대부분 MDP를 기반으로 합니다. Q-러닝은 MDP 환경에서 최적 정책을 학습하는 방법 중 하나입니다.

-  베이의 정리(Baye's theorem)란?
베이즈 정리 (Bayes' Theorem):

개념: 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 확률 이론의 기본 정리입니다. 새로운 정보(증거)를 얻었을 때, 어떤 사건에 대한 기존의 믿음(사전 확률)을 새로운 믿음(사후 확률)으로 갱신하는 방법을 제공합니다. 

수식: P(A∣B)= P(B∣A)P(A)/P(B)
​
 
P(A∣B): B라는 정보가 주어졌을 때 A의 확률 (사후 확률)
P(A): A의 사전 확률 (기존 믿음)
P(B∣A): A가 사실일 때 B가 관측될 확률 (우도, likelihood)
P(B): B가 관측될 확률 (증거)
관련성: 논문에서는 베이즈 정리가 모델링 및 파라미터 값과 관련된 불확실성을 고려하는 데 유용하다고 언급합니다. 
[source: 15] 베이지안 접근법의 수학적 기초가 됩니다.
-  최대우도법(Maximum likelihood)이란?
최대우도법 (Maximum Likelihood Estimation, MLE):

개념: 통계 모델의 파라미터(모수)를 추정하는 방법 중 하나입니다. 주어진 데이터(관측값)가 나타날 가능성(우도, likelihood)을 가장 크게 하는 파라미터 값을 찾는 방식입니다. 즉, "어떤 파라미터 값이 이 데이터를 가장 잘 설명하는가?"에 답하는 방법입니다.
관련성: 논문에서는 파라미터 값을 고정해야 하는 대부분의 의사결정 분석에서 최대우도법이 주로 사용된다고 언급합니다. [source: 16]


-  베이지안 접근법(Bayesian approach)이란 ?
베이지안 접근법 (Bayesian Approach):

개념: 통계적 추론에 대한 접근 방식 중 하나로, 베이즈 정리를 핵심적으로 사용합니다. 모르는 파라미터를 고정된 상수가 아닌 확률 변수로 간주하고, 이에 대한 사전 지식(prior information)을 확률 분포(사전 분포)로 표현합니다. 데이터를 관찰한 후, 이 사전 분포와 데이터의 우도를 베이즈 정리를 이용해 결합하여 파라미터에 대한 새로운 확률 분포(사후 분포)를 얻습니다.
관련성: 논문에서는 베이지안 접근법이 사전 정보를 활용하여 개별 파라미터의 범위를 지정할 수 있다는 장점을 가진다고 설명합니다. [source: 17] 이는 불확실성을 명시적으로 다루는 데 유용합니다.

-  비모수적(Non-parameter based) 의사결정 모델링이란?
}

